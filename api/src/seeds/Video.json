{
  "seed": [
    {
      "id": "795be3ee-bb98-4e11-854e-ef407fe46218",
      "title": "Generalizing to New Situations in Robotics by Bowen Li",
      "description": "This presentation, titled 'Generalizing to New Situations in Robotics', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/2454ca74-2b04-405e-b2c3-b2814473ba26.mp4",
      "duration": "45:47",
      "views": 145230,
      "likes": 4120,
      "dislikes": 32,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2025-04-19T13:51:28Z",
      "tags": ["generalizing", "new", "situations", "robotics"],
      "event": "SS2025"
    },
    {
      "id": "3267a66a-2800-4d6f-8640-ae7650b879aa",
      "title": "Why Current Models Will Always Hallucinate by Changlong Wu",
      "description": "This presentation, titled 'Why Current Models Will Always Hallucinate', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/58e190b0-79fd-4c1d-b9c1-bb1bed30daf2.mp4",
      "duration": "45:28",
      "views": 76540,
      "likes": 2345,
      "dislikes": 16,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2025-04-19T13:55:15Z",
      "tags": ["why", "current", "models", "will", "always", "hallucinate"],
      "event": "SS2025"
    },
    {
      "id": "cbdf7a3b-0a93-4f41-9b47-0d777cfa5145",
      "title": "Foundations of Interpretable Models by Mateo Zarlenga and Pietro Barbiero",
      "description": "This presentation, titled 'Foundations of Interpretable Models', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/7fa212d5-edca-4155-8bcd-66d306a69c55.mp4",
      "duration": "53:15",
      "views": 112890,
      "likes": 3987,
      "dislikes": 31,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2025-04-19T13:57:56Z",
      "tags": ["foundations", "interpretable", "models"],
      "event": "SS2025"
    },
    {
      "id": "36bbb85e-5df8-4e2f-8d32-4ce7eb58e94f",
      "title": "Neuro-Symbolic AI for Safer Autonomous Vehicles by Leilani Gilpin",
      "description": "This presentation, titled 'Neuro-Symbolic AI for Safer Autonomous Vehicles', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/cdc6f92e-1636-4c55-bd0e-743e6a5fb4c4.mp4",
      "duration": "51:48",
      "views": 89420,
      "likes": 2890,
      "dislikes": 18,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2024-12-14T02:52:41Z",
      "tags": ["neuro-symbolic", "ai", "safer", "autonomus", "vehicles"],
      "event": "SS2025"
    },
    {
      "id": "dfa223d1-cbe8-4fb0-823e-d83f6125aa20",
      "title": "How to Measure Compositionality, and Why it Leads to Better Generalization",
      "description": "This presentation, titled 'How to Measure Compositionality, and Why it Leads to Better Generalization', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/c621e659-5f17-4238-9cb9-d92071d86017.mp4",
      "duration": "49:28",
      "views": 127850,
      "likes": 3654,
      "dislikes": 23,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2024-12-14T02:52:41Z",
      "tags": [
        "how",
        "measure",
        "compositionality",
        "why",
        "leads",
        "better",
        "generalization"
      ],
      "event": "SS2025"
    },
    {
      "id": "07e3e39c-9bdd-4f41-80b7-150260cf53ff",
      "title": "Compositional Learning in Language and Vision by Parisa Kordjamshidi",
      "description": "This presentation, titled 'Compositional Learning in Language and Vision', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/a4d79861-4efe-49d3-aa50-a9fe562328c5.mp4",
      "duration": "56:35",
      "views": 198340,
      "likes": 6890,
      "dislikes": 45,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2024-12-14T02:52:41Z",
      "tags": ["compositional", "learning", "language", "vision"],
      "event": "SS2025"
    },
    {
      "id": "5",
      "title": "Why Neural Networks Can Discover Symbolic Structures by Peihao Wang",
      "description": "This presentation, titled 'Why Neural Networks Can Discover Symbolic Structures', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/19aa618b-a04a-40c5-9279-c9bceb24dcf3.mp4",
      "duration": "52:38",
      "views": 156780,
      "likes": 4320,
      "dislikes": 28,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2024-12-14T02:52:41Z",
      "tags": [
        "why",
        "neural",
        "networks",
        "can",
        "discover",
        "symbolic",
        "structures"
      ],
      "event": "SS2025"
    },
    {
      "id": "e71bfe9d-0f11-4e79-b559-89d5fe5c3060",
      "title": "Explainability While Retaining Predictive Accuracy by Steve Carrow, Olga Vilenskaia and Kevin OConnor",
      "description": "This presentation, titled 'Explainability While Retaining Predictive Accuracy', offers an in‑depth exploration of its central theme. It explains the core challenges, discusses current research directions, and highlights practical implications for real‑world applications. Through clear examples and expert insights, the talk aims to advance understanding and spark further inquiry within the field.",
      "thumbnail": "/public/logo.png",
      "videoUrl": "https://cdn.centaurinstitute.org/media/c6de85c6-576c-4417-b23b-44501ec2146a.mp4",
      "duration": "51:36",
      "views": 94580,
      "likes": 2876,
      "dislikes": 15,
      "channelName": "Centaur AI Institute",
      "channelAvatar": "https://yt3.ggpht.com/D0y_UHqjtyCfASentvqAFpCLw3qkKNrIL7uVl5Q15VoFzPVI_aqECdeZKDzjuG7EKfnp3cwH=s88-c-k-c0x00ffffff-no-rj",
      "uploadDate": "2024-12-14T02:52:41Z",
      "tags": [
        "explainability",
        "while",
        "retaining",
        "predictive",
        "accuracy"
      ],
      "event": "SS2025"
    }
  ]
}
